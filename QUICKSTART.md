# ğŸš€ MLLM Security - Quick Run Guide

## Fastest Way to Get Started

### Step 1: Activate Environment
```powershell
.\guardrails_env\Scripts\Activate.ps1
```

### Step 2: Install Missing Dependencies (if needed)
```powershell
pip install accelerate huggingface-hub
```

### Step 3: Login to Hugging Face
```powershell
huggingface-cli login
```
Get your token from: https://huggingface.co/settings/tokens

### Step 4: Run Quick Start
```powershell
python src\quickstart.py
```

---

## What Each Option Does

### Option 1: Train Model (Required First!)
- Downloads BeaverTails dataset (~150MB)
- Fine-tunes DistilBERT for toxicity detection
- Takes 10-30 minutes
- Creates `models/toxicity_classifier/`

### Option 2: Evaluate Model
- Tests model on held-out test set
- Generates metrics (accuracy, precision, recall, F1)
- Creates visualization plots in `results/`

### Option 3: Run Demo
- Shows guardrail pipeline in action
- Tests on example texts
- Displays confidence scores

### Option 4: Interactive Testing
- Type your own text to test
- Get instant safety predictions
- Great for experimentation

---

## Manual Commands (Alternative)

### Train
```powershell
python src\train_toxicity.py
```

### Evaluate
```powershell
python src\evaluate_model.py
```

### Demo
```powershell
python src\guardrail_pipeline.py
```

---

## Expected Results

- **Training Accuracy**: 85-95%
- **Test F1 Score**: 0.80-0.90
- **Inference Speed**: ~100-500 texts/second (CPU)

---

## Troubleshooting

### "Model not found"
â†’ Run Option 1 to train the model first

### "Not logged in to Hugging Face"
â†’ Run `huggingface-cli login`

### "Import errors"
â†’ Make sure you activated the environment:
```powershell
.\guardrails_env\Scripts\Activate.ps1
```

### "Out of memory"
â†’ In `train_toxicity.py`, change `batch_size=8` to `batch_size=4`

---

## Next Steps After Training

1. âœ… Train model (Option 1)
2. âœ… Evaluate performance (Option 2)
3. âœ… Try interactive testing (Option 4)
4. ğŸ“ Use in your own code (see README.md "Use in Your Code" section)
5. ğŸ”§ Adjust threshold in `GuardrailPipeline(threshold=0.7)` for stricter/looser filtering

---

## Project Structure

```
MLLM_security/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ quickstart.py          â­ START HERE
â”‚   â”œâ”€â”€ train_toxicity.py      # Training
â”‚   â”œâ”€â”€ evaluate_model.py      # Evaluation
â”‚   â”œâ”€â”€ guardrail_pipeline.py  # Inference
â”‚   â””â”€â”€ utils.py               # Helpers
â”œâ”€â”€ models/                    # Saved models (created after training)
â”œâ”€â”€ data/                      # Cached datasets
â”œâ”€â”€ results/                   # Plots and outputs
â””â”€â”€ README.md                  # Full documentation
```

---

**Need help?** Check the full README.md for detailed documentation and code examples!
